{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2acf12-04ce-4c28-ba09-eb0afce60c8b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<style>\n",
    ".jp-MarkdownOutput {\n",
    "    font-size: 2.5em !important;\n",
    "}\n",
    ".jp-MarkdownOutput table {\n",
    "    font-size: 0.75em !important;\n",
    "}\n",
    ".jp-OutputArea-output pre {\n",
    "    font-size: 2em !important;\n",
    "}\n",
    ".cm-content {\n",
    "    font-size: 2em !important;\n",
    "}\n",
    ".page-id-xx, html {\n",
    "    scrollbar-width: none; /* FF */\n",
    "}\n",
    "::-webkit-scrollbar {\n",
    "    width: 0px; /* Chrome & Edge */\n",
    "}\n",
    ".jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {\n",
    "    background: none;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e01471-98c3-437a-97b8-ca867f876422",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Adaptive basis functions\n",
    "\n",
    "### The missing link between linear fitting and neural networks\n",
    "\n",
    "<img src=\"img/sasquatch.jpg\" style=\"width: 600px; max-width: 80%; margin: 30px auto 30px auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83acce8f-4d4d-47c9-a104-6287b432e8f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "I know of three different ways to introduce neural networks:\n",
    "\n",
    "1. by analogy with biological neurons and human brains,\n",
    "2. as a generalization of linear fitting (this talk),\n",
    "3. \"shut up and calculate,\" just empirically showing that it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc8f7f-93e4-4830-9d7c-0082d2a5dffd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b143b0-0f21-4f4e-9bf8-968b9d56494b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "(1) is unsatisfactory because I don't know how brains work and it tempts anthropomorphization.\n",
    "\n",
    "(2) might be too mathematical for some audiences, but not this one.\n",
    "\n",
    "(3) is deeply unsatisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28815247-77dd-4cbe-838e-312132b86479",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "You probably already know this material, but the point is the story arc, how we get from linear fitting to feed-forward neural networks, so please be patient!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb2596-fef6-430f-9f3d-f969c0406101",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Linear fitting\n",
    "\n",
    "We're given pairs of values, $x_i$ and $y_i$ (indexes $i \\in [0, N)$ for $N$ pairs) that have an approximately linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9dc50-4a88-4d36-aa7f-f614c903d433",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768fa01f-a4f9-4d9e-b86f-76e83015c1dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab91fdd-a583-4345-a7d8-3d56118ba362",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uniformly random x values\n",
    "x = np.random.uniform(-5, 5, 100)\n",
    "\n",
    "# linear (2 * x + 3) plus normal-distributed noise\n",
    "y = (2 * x + 3) + np.random.normal(0, 3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c25ba-1485-452b-b003-4673de524769",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee3f06-86d7-4a06-a8c8-e7c5794492c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4904c68-3410-4b41-90d9-239ba68beafa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, marker=\"+\", color=\"tab:orange\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73038ae9-b687-4fbe-9092-71465b7a2aa7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We can write an equation for an exact relationship:\n",
    "\n",
    "$$ a \\, x + b = y $$\n",
    "\n",
    "and express the goodness of this model as a fit to the data by how close it gets to the actual points:\n",
    "\n",
    "$$ \\chi^2 = \\sum_i \\bigg| (a \\, x_i + b) - y_i \\bigg|^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e8707-77fd-41cd-84d1-b7d8f88687e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "If $a$ and $b$ are chosen well, the line will go through the middle of the points.\n",
    "\n",
    "<img src=\"img/equation-for-a-line.svg\" style=\"width: 600px; max-width: 80%; margin: 30px auto 30px auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697df14f-74ce-4cd2-8c61-33429eb4ab44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The best $a$ and $b$ are the values in which $\\chi^2$ is minimized. Using calculus, one can find an exact minimum; here's the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272290a-9da3-426f-b97d-375bd41464d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum1  = len(x)\n",
    "sumx  = np.sum(x)\n",
    "sumy  = np.sum(y)\n",
    "sumxx = np.sum(x**2)\n",
    "sumxy = np.sum(x * y)\n",
    "delta = (sum1*sumxx) - (sumx*sumx)\n",
    "\n",
    "a = ((sum1*sumxy) - (sumx*sumy))  / delta\n",
    "b = ((sumxx*sumy) - (sumx*sumxy)) / delta\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596a49e-9986-42f5-a19f-41dc85c28124",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e15c50-feba-4b04-9f6c-c334594b327a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Since the data were generated from\n",
    "\n",
    "$$y_i = (a \\, x + b) + \\mbox{noise}$$\n",
    "\n",
    "for $a = 2$ and $b = 3$, the best fit values of $a$ and $b$ should be _close to_ 2 and 3 (not exact because of the noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2c102-9ae2-4bf2-b9e7-08d84d4d1af4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, marker=\"+\", color=\"tab:orange\")\n",
    "\n",
    "ax.plot([-5, 5], [a*-5 + b, a*5 + b], color=\"tab:blue\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c707af2f-7539-4c6e-904a-67a342ed14c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Most problems have more than one independent variable ($x$). For problems like that, we can replace scalar $a$ and $x$ with vectors and use $\\cdot$ for matrix multiplication:\n",
    "\n",
    "$$\\left(\\begin{array}{c c}\n",
    "a^1 & a^2 \\\\\n",
    "\\end{array}\\right) \\cdot \\left(\\begin{array}{c}\n",
    "x^1 \\\\\n",
    "x^2 \\\\\n",
    "\\end{array}\\right) + b = y$$\n",
    "\n",
    "<img src=\"img/equation-for-a-plane.svg\" style=\"width: 600px; max-width: 80%; margin: 30px auto 30px auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe71645-2bf3-4638-9ea0-c4cc8cc804e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "For an arbitrary number of dimensions:\n",
    "\n",
    "$$\\left(\\begin{array}{c c c c}\n",
    "a^1 & a^2 & \\cdots & a^n \\\\\n",
    "\\end{array}\\right) \\cdot \\left(\\begin{array}{c}\n",
    "x^1 \\\\\n",
    "x^2 \\\\\n",
    "\\vdots \\\\\n",
    "x^n\n",
    "\\end{array}\\right) + b = y$$\n",
    "\n",
    "(No picture. üòû)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88514a9c-d952-4ead-bd00-2a179eb84bef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Many problems also have more than one given variable. We can replace $y$ and $b$ with vectors $\\vec{y}$ and $\\vec{b}$, and that requires $\\hat{a}$ to be a full matrix.\n",
    "\n",
    "$$\\left(\\begin{array}{c c c c}\n",
    "a^{1,1} & a^{1,2} & \\cdots & a^{1,n} \\\\\n",
    "a^{2,1} & a^{2,2} & \\cdots & a^{2,n} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "a^{m,1} & a^{m,2} & \\cdots & a^{m,n} \\\\\n",
    "\\end{array}\\right) \\cdot \\left(\\begin{array}{c}\n",
    "x^1 \\\\\n",
    "x^2 \\\\\n",
    "\\vdots \\\\\n",
    "x^n\n",
    "\\end{array}\\right) + \\left(\\begin{array}{c}\n",
    "b^1 \\\\\n",
    "b^2 \\\\\n",
    "\\vdots \\\\\n",
    "b^m\n",
    "\\end{array}\\right) = \\left(\\begin{array}{c}\n",
    "y^1 \\\\\n",
    "y^2 \\\\\n",
    "\\vdots \\\\\n",
    "y^m\n",
    "\\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f9999-c1ff-4aaa-9005-abb87d790df0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Side-note: we can also reduce this to a single matrix multiplication (no addition) by adding a fake dimension to $\\vec{x}$ whose value is always $1$ and move the $\\vec{b}$ into a column of $\\hat{a}$.\n",
    "\n",
    "$$\\left(\\begin{array}{c c c c c}\n",
    "a^{1,1} & a^{1,2} & \\cdots & a^{1,n} & b^1 \\\\\n",
    "a^{2,1} & a^{2,2} & \\cdots & a^{2,n} & b^2 \\\\\n",
    "\\vdots & \\vdots & & \\vdots & \\vdots \\\\\n",
    "a^{m,1} & a^{m,2} & \\cdots & a^{m,n} & b^m \\\\\n",
    "\\end{array}\\right) \\cdot \\left(\\begin{array}{c}\n",
    "x^1 \\\\\n",
    "x^2 \\\\\n",
    "\\vdots \\\\\n",
    "x^n \\\\\n",
    "1\n",
    "\\end{array}\\right) = \\left(\\begin{array}{c}\n",
    "y^1 \\\\\n",
    "y^2 \\\\\n",
    "\\vdots \\\\\n",
    "y^m\n",
    "\\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a4ccf-6da4-47e8-8bb7-638f638da93a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Goodness of fit can still be expressed by a $\\chi^2$,\n",
    "\n",
    "$$\\chi^2 = \\sum_i \\bigg\\| (\\hat{a} \\cdot \\vec{x}_i + \\vec{b}) - \\vec{y}_i \\bigg\\|^2$$\n",
    "\n",
    "and the solution is still exact, though [the derivation](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) involves more intense linear algebra.\n",
    "\n",
    "$$\\hat{A} = (\\hat{X}^T \\, \\hat{X})^{-1} \\, \\hat{X}^T \\, \\hat{Y}$$\n",
    "\n",
    "where\n",
    "* $\\hat{X}$ is the matrix of $\\vec{x}_i$ features (with the \"fake\" dimension)\n",
    "* $\\hat{Y}$ is a matrix of $\\vec{y}_i$ predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca306271-ccc4-4283-baa3-b78844341f70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "This is the form in which fitting libraries expect the problem to be cast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8813a5b-3de7-4137-b895-035c9aed8881",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53791556-65b0-46af-976b-0ec3ed40735c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c95256-6cef-4245-8adf-84ed98e99aa6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scikit-Learn wants X to be an array of vectors, even if they're 1-dimensional\n",
    "X = x[:, np.newaxis]\n",
    "Y = y\n",
    "\n",
    "best_fit = LinearRegression().fit(X, y)\n",
    "\n",
    "(a,) = best_fit.coef_\n",
    "b = best_fit.intercept_\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992364c0-6755-4ddd-b0a5-9ab20d5b1ca3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Or with more dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b9d581-96d0-4004-9bc0-ca88cc452702",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uniformly random 2-D vectors\n",
    "X = np.random.uniform(-5, 5, (100000, 2))\n",
    "\n",
    "# true linear relationship (2-D ‚Üí 3-D)\n",
    "a_true = np.array([[1.1, 2.2],\n",
    "                   [3.3, 4.4],\n",
    "                   [5.5, 6.6]])\n",
    "b_true = np.array([7.7, 8.8, 9.9])\n",
    "\n",
    "# linear (a_true ¬∑ b_true) for each x ‚àà X plus noise\n",
    "Y = (X @ a_true.T + b_true) + np.random.normal(0, 1, (100000, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce0b5e-b8d5-4713-bc1f-d074cb9e7622",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa3a79-260b-4263-b73b-2f46e9b4a110",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_fit = LinearRegression().fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53335d70-5828-4a3c-b95b-de537fd695d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585e216-05c2-4635-b089-30690ee1bef1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_fit.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b7f2a-3ad0-45e0-8a72-2e2df6a0c61e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f0f581-3343-483c-bc05-ceff9d7f1153",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_fit.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3469ea-f288-432f-be1d-f0f49fce65d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Beyond linear fitting\n",
    "\n",
    "Many problems have relationships between features ($x$) and predictions ($y$) that are not linear, so we'll need to generalize.\n",
    "\n",
    "How we generalize depends critically on whether we know the _shape_ of the true relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e34454-3c62-4d46-9bfc-28973ddf4101",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Example:** an object tossed through the air has an altitude versus time curve, $y(t)$, like this:\n",
    "\n",
    "$$y(t) = y(t; \\, \\underbrace{y_0, \\, t_0, \\, \\mu, \\, t_f}_{\\mbox{parameters}}) = y_0 - \\frac{1}{\\mu} \\log \\left( \\cosh \\frac{t - t_0}{t_f} \\right)$$\n",
    "\n",
    "where $y_0$ and $t_0$ are the starting position and time, and $\\mu$ and $t_f$ are related to the air resistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11c626-a0da-4574-99af-8e85597328b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def y_true(t):\n",
    "    y0, t0, mu, tf = 60, 3, 0.05, 2\n",
    "    return y0 - (1/mu)*np.log(np.cosh((t - t0)/tf))\n",
    "\n",
    "def measurement_error(n):\n",
    "    return np.random.normal(0, 1, n)\n",
    "\n",
    "t = np.linspace(0, 10, 50)\n",
    "y = y_true(t) + measurement_error(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8ebd9-3222-4b6b-8598-dbd9aef2e8ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(t, y, marker=\".\", color=\"tab:orange\")\n",
    "\n",
    "ax.set_xlabel(\"time after release\")\n",
    "ax.set_ylabel(\"height above ground\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b086b-f0ef-4747-b60a-5d043c6c415b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A linear fit would be a disaster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28dbb1-a07a-45e6-891e-55abfe685e36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_fit = LinearRegression().fit(t[:, np.newaxis], y)\n",
    "(linear_slope,), linear_intercept = best_fit.coef_, best_fit.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcab28f-992e-4216-b5d1-5f844f996fde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5d3e0-26c0-46ac-94db-7240458827c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot([0, 10], [linear_slope * 0 + linear_intercept,\n",
    "                  linear_slope * 10 + linear_intercept])\n",
    "ax.scatter(t, y, marker=\".\", color=\"tab:orange\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7655f23-ecc3-4684-bfe6-806e12f5726d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Knowing the shape, $y(t; \\, y_0, \\, t_0, \\, \\mu, \\, t_f)$, we can define a goodness of fit as before:\n",
    "\n",
    "$$\\chi^2 = \\sum_i \\bigg| y(t_i; \\, y_0, \\, t_0, \\, \\mu, \\, t_f) - y_i \\bigg|^2$$\n",
    "\n",
    "and find the parameters $y_0$, $t_0$, $\\mu$, $t_f$ that minimize $\\chi^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd675b-59e8-4316-8358-3773691d2d08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1fb382-97f3-4d44-8acc-89641a7a8af2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "However, this would be a different calculus problem for every shape, and may be impossible for some."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906032f-5ff3-4eae-9601-65cf3be60173",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Particle physicist's solution: write a general numerical minimizer and use it on all problems everywhere (üî®).\n",
    "\n",
    "The [Minuit](https://doi.org/10.1016/0010-4655(75)90039-9) package was written in 1975 and is still the default tool for most fitting problems.\n",
    "\n",
    "<img src=\"img/minuit-turned-50.png\" style=\"width: 1200px; max-width: 100%; margin: 50px auto 30px auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904795cc-e774-4e16-9d0d-7e58d365e546",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from iminuit import Minuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db9145-eb0f-4872-82bc-7b4fccc24b50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def y_of_t_parameterized(t, y0, t0, mu, tf):\n",
    "    return y0 - (1/mu)*np.log(np.cosh((t - t0)/tf))\n",
    "\n",
    "def chi2(y0, t0, mu, tf):\n",
    "    return np.sum((y_of_t_parameterized(t, y0, t0, mu, tf) - y)**2)\n",
    "\n",
    "minimizer = Minuit(chi2, y0=100, t0=0, mu=0.1, tf=3)   # ‚Üê initial guess of parameters\n",
    "minimizer.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b5bce-7fca-4231-860e-31ebe5f61867",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115facbd-8e2f-42c0-9109-a125ec7bc523",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "{p.name: p.value for p in minimizer.params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f67f22f-e652-4169-805b-8f63b160776a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_of_t = y_of_t_parameterized(t, **{p.name: p.value for p in minimizer.params})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2826662-c7c8-48f7-9201-ac13f3f9769c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79297ea-bc95-4297-948f-e4d8f88c073b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(t, y_of_t)\n",
    "ax.scatter(t, y, marker=\".\", color=\"tab:orange\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ee625-ded7-46fa-8d6a-ffe633b40944",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Great! But what if we assumed the formula for an object tossed _without_ air resistance?\n",
    "\n",
    "$$y(t) = y_0 - \\frac{1}{2}g(t - t_0)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5256d14-63fe-40ad-8599-c441190578b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def y_of_t_wrong_shape(t, y0, t0, g):\n",
    "    return y0 - (1/2)*g*(t - t0)**2\n",
    "\n",
    "def chi2_wrong_shape(y0, t0, g):\n",
    "    return np.sum((y_of_t_wrong_shape(t, y0, t0, g) - y)**2)\n",
    "\n",
    "minimizer_wrong_shape = Minuit(chi2_wrong_shape, y0=100, t0=0, g=5)\n",
    "minimizer_wrong_shape.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f043c5-e45e-4a79-ab4b-f25a73ec6963",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_of_t = y_of_t_wrong_shape(t, **{p.name: p.value for p in minimizer_wrong_shape.params})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d0fed-b50d-4d5b-92ab-55a5931a9ce8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d28a3-6c81-42cb-8576-1c2f0f19ba0d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(t, y_of_t)\n",
    "ax.scatter(t, y, marker=\".\", color=\"tab:orange\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93225372-d8fb-421d-8700-d84949570b1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Or even just a bad initial guess of the parameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a2588-fe6f-44a0-b962-22fd5f7ee69e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "minimizer_bad_guess = Minuit(chi2, y0=100, t0=100, mu=100, tf=100)   # ‚Üê initial guess\n",
    "minimizer_bad_guess.migrad()\n",
    "\n",
    "y_of_t = y_of_t_parameterized(t, **{p.name: p.value for p in minimizer_bad_guess.params})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec426b-bd39-4b9d-b18e-00edb9bab262",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce12d3-f1df-4944-90ba-80145657918f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(t, y_of_t)\n",
    "ax.scatter(t, y, marker=\".\", color=\"tab:orange\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0fd590-31d5-476d-bf6c-55151678fd2a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The Minuit approach is popular because the goal of physics is to model the world with exact formulas.\n",
    "\n",
    "But for any problems involving complex systems, especially human decisions, behavior can't be derived from first principles.\n",
    "\n",
    "So we need a way to fit functions without knowing what their shapes are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c31bb-8b8e-4ab5-98ca-c0000015eadc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Universal approximators\n",
    "\n",
    "**Classical:**\n",
    "* Taylor and orthogonal polynomials (Legendre, Jacobi, Laguerre, Hermite, Chebyshev...)\n",
    "* Fourier series and transformations\n",
    "\n",
    "**Neural networks in disguise:**\n",
    "* Adaptive basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c43bb-da0b-4e44-98dd-321529338256",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In many contexts, polynomial series are the \"go to\" universal approximators:\n",
    "\n",
    "$$ f(x) \\approx f^{N_{\\mbox{terms}}}(x) = \\sum_n^{N_{\\mbox{terms}}} a_n \\, x^n $$\n",
    "\n",
    "with suitably chosen $a_n$ (for $n \\in [0, {N_{\\mbox{terms}}})$). Higher cut-offs $N_{\\mbox{terms}}$ yield better approximatoins.\n",
    "\n",
    "<img src=\"img/Logarithm_GIF.gif\" style=\"max-width: 80%; margin: 0 auto 0 auto\">\n",
    "\n",
    "<span style=\"font-size: 0.5em;\">Credit: <a href=\"https://commons.wikimedia.org/w/index.php?curid=27892918\">IkamusumeFan - Own work, CC BY-SA 3.0</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690979ec-31e5-4ec3-87e7-859299bddc12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "On a restricted domain like $-1 < x < 1$, there's a series of Legendre polynomials $P_n(x)$ that are orthogonal to one another:\n",
    "\n",
    "$$ \\int_{-1}^1 P_n(x) \\, P_m(x) \\, dx = 0 \\mbox{\\hspace{1 cm} if $n \\ne m$} $$\n",
    "\n",
    "$P_0(x) = 1$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_1(x) = x$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_2(x) = \\frac{1}{2}(3x^2 - 1)$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_3(x) = \\frac{1}{2}(5x^3 - 3x)$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7012c832-8d53-4782-9334-89f478b5ea99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94461e2-e7ce-40e0-a785-3c4fff9bd1ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "For a function\n",
    "\n",
    "$$ f^{N_{\\mbox{polynomials}}}(x) = \\sum_n^{N_{\\mbox{polynomials}}} a_n \\, P_n(x) \\mbox{\\hspace{1 cm} that approximates \\hspace{1 cm}} y_i \\approx f^{N_{\\mbox{polynomials}}}(x_i), $$\n",
    "\n",
    "each $a_n$ can be computed independently of all other $a_m$ by integration.\n",
    "\n",
    "This series of polynomials is an _orthogonal basis_ for the space of smooth functions on $-1 < x < 1$: you can make any shape by adding them with the right combination of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1ba63-c8f7-4eb5-afa6-1e49270f0c48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "But since orthogonal polynomial series are all linear combinations of simple powers,\n",
    "\n",
    "$p_0(x) = 1$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$p_1(x) = x$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$p_2(x) = x^2$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$p_3(x) = x^3$,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Taylor series)\n",
    "\n",
    "we can be lazy and just fit a curve to $p_n(x)$. (Their best-fit coefficients will be correlated.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef0502-98a3-4da1-890a-89f308a6821e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ae3f6-42c4-4c3d-9f33-cf92549f64ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.vstack([[np.ones_like(t)], [t], [t**2], [t**3]]).T\n",
    "best_fit = LinearRegression(fit_intercept=False).fit(X, y)\n",
    "\n",
    "y_of_t = np.sum(X * best_fit.coef_[np.newaxis, :], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0a348-c5d7-4513-92d7-b691322ca3a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54995059-6006-4e28-a09e-f7e42d9336ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(t, y_of_t)\n",
    "ax.scatter(t, y, marker=\".\", color=\"tab:orange\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a94dc5-603e-4883-bd1e-4c996258ba28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's do this with a trickier problem:\n",
    "\n",
    "$$y = \\left\\{\\begin{array}{l l}\n",
    "\\sin(22 x) & \\mbox{if } |x - 0.43| < 0.15 \\\\\n",
    "-1 + 3.5 x - 2 x^2 & \\mbox{otherwise} \\\\\n",
    "\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766e3b8-1ebb-4972-9f3a-f641993832dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def truth(x):\n",
    "    return np.where(abs(x - 0.43) < 0.15, np.sin(22*x), -1 + 3.5*x - 2*x**2)\n",
    "\n",
    "x = np.random.uniform(0, 1, 1000)\n",
    "y = truth(x) + np.random.normal(0, 0.03, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562522a5-b95b-45e8-80dc-2cb99474810a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "curve_x = np.linspace(0, 1, 1000)\n",
    "curve_y = truth(curve_x)\n",
    "\n",
    "ax.scatter(x, y, marker=\".\", label=\"data to fit\")\n",
    "ax.plot(curve_x, curve_y, color=\"magenta\", linewidth=3, label=\"truth\")\n",
    "\n",
    "ax.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce9e30-68a3-40ec-aa09-76a111995d5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_POLYNOMIAL_TERMS = 15\n",
    "\n",
    "coefficients = np.polyfit(x, y, NUMBER_OF_POLYNOMIAL_TERMS - 1)[::-1]\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "model_y = sum(c * model_x**i for i, c in enumerate(coefficients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bbebb4-c2bd-4c7a-89b5-055ef78d194d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac2fe4-c0b1-4bc9-a332-ba799f8131a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, marker=\".\")\n",
    "ax.plot(curve_x, curve_y, color=\"magenta\", linewidth=3)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.legend([\"measurements\", \"truth\", f\"{len(coefficients)} Taylor components\"], loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a097bac-5a47-4159-a504-bdb2a693e24f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Fourier series are another classical universal approximator, built from of sines and cosines over some period $P$, rather than polynomials of $x$.\n",
    "\n",
    "$$\\begin{array}{c}\n",
    "f(x) = a_0 + a_1 \\cos\\left(2\\pi\\frac{1}{P}x\\right) + b_1 \\sin\\left(2\\pi\\frac{1}{P}x\\right) + \\\\ \\hspace{7 cm} a_2 \\cos\\left(2\\pi\\frac{2}{P}x\\right) + b_2 \\sin\\left(2\\pi\\frac{2}{P}x\\right) + \\ldots \\end{array}$$\n",
    "\n",
    "They're also orthogonal, so each coefficient can be determined independently of the others.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_0 &&=&& \\frac{1}{P} \\int_P f(x) \\, dx \\\\\n",
    "a_n &&=&& \\frac{2}{P} \\int_P f(x) \\cos\\left(2\\pi\\frac{n}{P}x\\right) \\, dx \\\\\n",
    "b_n &&=&& \\frac{2}{P} \\int_P f(x) \\sin\\left(2\\pi\\frac{n}{P}x\\right) \\, dx \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e34c48-a991-47ff-86c3-8d925306e3d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_COS_TERMS = 7\n",
    "NUMBER_OF_SIN_TERMS = 7\n",
    "\n",
    "sort_index = np.argsort(x)\n",
    "x_sorted = x[sort_index]\n",
    "y_sorted = y[sort_index]\n",
    "\n",
    "constant_term = np.trapezoid(y_sorted, x_sorted)\n",
    "cos_terms = [2*np.trapezoid(y_sorted * np.cos(2*np.pi * (i + 1) * x_sorted), x_sorted)\n",
    "             for i in range(NUMBER_OF_COS_TERMS)]\n",
    "sin_terms = [2*np.trapezoid(y_sorted * np.sin(2*np.pi * (i + 1) * x_sorted), x_sorted)\n",
    "             for i in range(NUMBER_OF_SIN_TERMS)]\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "model_y = (\n",
    "    constant_term +\n",
    "    sum(coefficient * np.cos(2*np.pi * (i + 1) * model_x)\n",
    "        for i, coefficient in enumerate(cos_terms)) +\n",
    "    sum(coefficient * np.sin(2*np.pi * (i + 1) * model_x)\n",
    "        for i, coefficient in enumerate(sin_terms))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8de7e9-a92a-4558-af5f-61fea2dafe71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, marker=\".\")\n",
    "ax.plot(curve_x, curve_y, color=\"magenta\", linewidth=3)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.legend([\"measurements\", \"truth\", f\"{1 + len(cos_terms) + len(sin_terms)} Fourier components\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e7ffc-b39c-4ff7-a590-c478835128f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**What polygons and Fourier series have in common:** they all approximate a function with a _fixed set_ of basis functions $\\phi_n$ for $n \\in [0, N_{\\mbox{functions}})$.\n",
    "\n",
    "$$f(x) = \\sum_n^{N_{\\mbox{functions}}} c_n \\, \\psi_n(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd0d58-4c84-49b6-a394-b3e4fc3e9151",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c1ac13-f6b9-4b45-95c5-11ab5d075061",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Suppose, instead, that we had a set of functions that could also _change shape:_\n",
    "\n",
    "$$f(x) = \\sum_n^{N_{\\mbox{functions}}} c_n \\, \\psi(x; \\, \\underbrace{\\alpha_n, \\, \\beta_n}_{\\mbox{parameters}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b1efb-3674-4de3-9207-88fb0cb35c01",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d27e28-116b-4096-9c41-738dc88d2fd4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Rather than having to accumulate enough terms to get one with a wiggle in the right $x$ position, the fitter would be able to horizontally shift one of these $\\psi$ functions to that spot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38098ef4-90c0-491c-95c8-f5385f04efb4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Suppose, for example, that our set of functions $\\psi_n$ are sigmoids with adjustable center $\\alpha$ and width $\\beta$.\n",
    "\n",
    "$$\\psi(x; \\alpha, \\beta) = \\frac{1}{1 + \\exp\\left((x - \\alpha)/\\beta\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9b943-b7fd-4e54-ab98-4356d1a7aeb6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid_component(x, center, width):\n",
    "    with np.errstate(over=\"ignore\", divide=\"ignore\"):  # ignore NumPy warnings\n",
    "        return 1 / (1 + np.exp((x - center) / width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defaf066-3691-4e8e-a343-8896b1b4e5bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sample_x = np.linspace(0, 1, 1000)\n",
    "ax.plot(model_x, sigmoid_component(sample_x, 0.5, 0.2), label=r\"$\\alpha = 0.5$, $\\beta = 0.2$\")\n",
    "ax.plot(model_x, sigmoid_component(sample_x, 0.5, 0.1), label=r\"$\\alpha = 0.5$, $\\beta = 0.1$\")\n",
    "ax.plot(model_x, sigmoid_component(sample_x, 0.75, -0.01), label=r\"$\\alpha = 0.75$, $\\beta = -0.01$\")\n",
    "\n",
    "ax.legend(loc=\"lower left\", bbox_to_anchor=(0.05, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b6323d-7a1d-4460-9c89-7d40c0156323",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Adaptive basis functions are not orthogonal, so the best fit can't be solved by an analytic formula or an integration. Instead, we have to do a generic search (e.g. Minuit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d8374-a92d-4a2c-8a69-333b850f8f09",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_SIGMOIDS = 5\n",
    "\n",
    "def sigmoid_sum(x, parameters):\n",
    "    out = np.zeros_like(x)\n",
    "    for coefficient, center, width in parameters.reshape(-1, 3):\n",
    "        out += coefficient * sigmoid_component(x, center, width)\n",
    "    return out\n",
    "\n",
    "def least_squares(parameters):\n",
    "    return np.sum((sigmoid_sum(x, parameters) - y)**2)\n",
    "\n",
    "best_minimizer = None\n",
    "for iteration in range(15):  # best of 15 optimizations; this is a difficult fit\n",
    "    initial_parameters = np.zeros(5 * 3)\n",
    "    initial_parameters[0::3] = np.random.normal(0, 1, NUMBER_OF_SIGMOIDS)    # coefficient terms\n",
    "    initial_parameters[1::3] = np.random.uniform(0, 1, NUMBER_OF_SIGMOIDS)   # center parameters (alpha)\n",
    "    initial_parameters[2::3] = np.random.normal(0, 0.1, NUMBER_OF_SIGMOIDS)  # width parameters (beta)\n",
    "\n",
    "    minimizer = Minuit(least_squares, initial_parameters)\n",
    "    minimizer.migrad()\n",
    "\n",
    "    if best_minimizer is None or minimizer.fval < best_minimizer.fval:\n",
    "        best_minimizer = minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030519b-f807-47bd-93ec-37f5e7e1af47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "model_y_adaptive = sigmoid_sum(model_x, np.array(best_minimizer.values))\n",
    "\n",
    "ax.scatter(x, y, marker=\".\")\n",
    "ax.plot(curve_x, curve_y, color=\"magenta\", linewidth=3)\n",
    "ax.plot(model_x, model_y_adaptive, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.legend([\"measurements\", \"truth\", f\"{len(minimizer.parameters)} sigmoid parameters\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e25c7c6-34c6-4c67-a7c1-d028b2f3f506",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Not only is the fit better with 15 parameters, but it generalizes beyond the training data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbbce8-5089-4f78-8a76-68ee9c3ec216",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "curve_x = model_x = np.linspace(0, 2, 2000)\n",
    "curve_y = truth(curve_x)\n",
    "model_y_polynomial = sum(c * model_x**i for i, c in enumerate(coefficients))\n",
    "model_y_fourier = (\n",
    "    constant_term +\n",
    "    sum(coefficient * np.cos(2*np.pi * (i + 1) * model_x)\n",
    "        for i, coefficient in enumerate(cos_terms)) +\n",
    "    sum(coefficient * np.sin(2*np.pi * (i + 1) * model_x)\n",
    "        for i, coefficient in enumerate(sin_terms))\n",
    ")\n",
    "model_y_adaptive = sigmoid_sum(model_x, np.array(best_minimizer.values))\n",
    "\n",
    "ax.scatter(x, y, marker=\".\")\n",
    "ax.plot(curve_x, curve_y, color=\"magenta\", linewidth=3)\n",
    "ax.plot(model_x, model_y_polynomial, color=\"green\", linewidth=3)\n",
    "ax.plot(model_x, model_y_fourier, color=\"orange\", linewidth=3)\n",
    "ax.plot(model_x, model_y_adaptive, color=\"blue\", linewidth=3)\n",
    "\n",
    "ax.legend([\"measurements\", \"truth\",\n",
    "           f\"{len(coefficients)} Taylor components\",\n",
    "           f\"{1 + len(cos_terms) + len(sin_terms)} Fourier components\",\n",
    "           f\"{len(minimizer.parameters)} sigmoid parameters\"])\n",
    "ax.set_ylim(-1.5, 2.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a13b47f-0d4e-4dd6-8559-3b0c66f5977d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "That is, lacking any information, it limits to a constant, rather than shooting into space as a high-order polynomial or repeating the pattern as a cyclic function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f5671-17ce-4301-84ad-ca10a7071e25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "By varying the scale ($c_n$), center ($\\alpha_n$), and width ($\\beta_n$) of each sigmoid, we can build any sandcastle we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67a77b-5835-4a10-8f4f-f81fc322580b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "\n",
    "wide_plateau_left = sigmoid_component(model_x, 0.2, 0.005)\n",
    "wide_plateau_right = sigmoid_component(model_x, 0.9, -0.005)\n",
    "\n",
    "narrow_peak_left = sigmoid_component(model_x, 0.4, 0.005)\n",
    "narrow_peak_right = sigmoid_component(model_x, 0.6, -0.005)\n",
    "\n",
    "ax.plot(model_x, -wide_plateau_left - wide_plateau_right - narrow_peak_left - narrow_peak_right);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951ad56-9eb3-4ea0-80b9-1e9621b4189c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**What does this have to do with neural networks?**\n",
    "\n",
    "Conventional view of a neural network:\n",
    "\n",
    "<img src=\"img/artificial-neural-network-layers-6.svg\" style=\"width: 1000px; max-width: 80%; margin: 0 auto 0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e48970d-1422-46f7-8086-3c81ce3d787c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**What does this have to do with neural networks?**\n",
    "\n",
    "Conventional view of a neural network:\n",
    "\n",
    "<img src=\"img/artificial-neural-network-layers-7.svg\" style=\"width: 1000px; max-width: 80%; margin: 0 auto 0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb67f9-6822-40f4-a069-921c6d78f82d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Boxes are components of a layer's vector $\\vec{x}$ and arrows are terms in the matrix $\\hat{A}^{L1-L2}$ that transforms one layer to the next.\n",
    "\n",
    "$$ f_{\\mbox{activation}}\\left[\\left( \\begin{array}{c c c c c c}\n",
    "a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5} & b_1 \\\\\n",
    "a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5} & b_2 \\\\\n",
    "a_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5} & b_3 \\\\\n",
    "a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4} & a_{4,5} & b_4 \\\\\n",
    "\\end{array} \\right) \\cdot \\left( \\begin{array}{c}\n",
    "{x^{L1}}_1 \\\\\n",
    "{x^{L1}}_2 \\\\\n",
    "{x^{L1}}_3 \\\\\n",
    "{x^{L1}}_4 \\\\\n",
    "{x^{L1}}_5 \\\\\n",
    "1 \\\\\n",
    "\\end{array} \\right)\\right] = \\left(\\begin{array}{c}\n",
    "{x^{L2}}_1 \\\\\n",
    "{x^{L2}}_2 \\\\\n",
    "{x^{L2}}_3 \\\\\n",
    "{x^{L2}}_4 \\\\\n",
    "\\end{array}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192a5fa-c6b1-4eb6-b3ee-e519f9d78171",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A parameterized sigmoid,\n",
    "\n",
    "$$ \\psi(x; \\alpha_n, \\beta_n) = \\frac{1}{1 + \\exp\\left((x - \\alpha_n)/\\beta_n\\right)} $$\n",
    "\n",
    "can be written as a linear-transformed $x$ passed into a hard-coded sigmoid:\n",
    "\n",
    "$$\\frac{1}{1 + \\exp\\left(x'\\right)} \\mbox{\\hspace{1 cm} with \\hspace{1 cm}} x' = (x - \\alpha_n)/\\beta_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a790282-afe5-4399-900c-9b0f3ed3fdd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26024c-0a3f-463c-80ef-9d881e9b24fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Renaming things,\n",
    "\n",
    "$$ f_{\\mbox{activation}}(x) = \\frac{1}{1 + \\exp\\left( x \\right)} \\mbox{\\hspace{1 cm} and \\hspace{1 cm}} \\vec{x}^{L2} = \\hat{A}^{L1-L2} \\cdot \\vec{x}^{L1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93356edd-8040-466f-801c-212d6537b704",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Visualizing $N_{\\mbox{functions}} = 5$ adaptive sigmoids in this way,\n",
    "\n",
    "<img src=\"img/artificial-neural-network-layers-3.svg\" style=\"width: 1000px; max-width: 80%; margin: 0 auto 0 auto\">\n",
    "\n",
    "The first set of arrows is shifting the centers and widths of the sigmoids horizontally, and the second set of arrows is multiplying each sigmoid by a coefficient vertically, then adding them together with a bias term ($y_0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd4a72-93a1-4506-9697-70b4807b466c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a233bc-fb61-49bb-8552-0632b832eee6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The purpose of a hidden layer is to implement one set of adaptive basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ef0ed-d484-4433-a2cb-9edd017d2eca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "This is why the Universal Approximation Theorem applies to _one layer_ of a neural network.\n",
    "\n",
    "Just as a sum of\n",
    "\n",
    "$$ a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 \\ldots $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ a_0 + a_1 \\cos(x) + b_1 \\sin(x) + a_2 \\cos(2x) + b_2 \\sin(2x) + \\ldots $$\n",
    "\n",
    "can approximate any bounded function with sufficiently many terms,\n",
    "\n",
    "$$ c_0 \\, f_{\\mbox{activation}}(a_0 \\, x + b_0) + c_1 \\, f_{\\mbox{activation}}(a_1 \\, x + b_1) + \\ldots $$\n",
    "\n",
    "can approximate any function with sufficiently many terms (sufficiently many nodes in the hidden layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ced5a-b7fc-403a-8e5f-fd30f99c796a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "However, if the shape of the training data has a large number of wiggles, a single-hidden layer network must use up that many adaptive basis functions (hidden layer nodes) to follow the trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6b4ee-a533-4e59-9975-56e823588dad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d1f29-af49-405a-a07b-21c1e1ea4849",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Multiple hidden layers non-linearly transform the space for the next layer, like this:\n",
    "\n",
    "<img src=\"img/deep-learning-by-space-folding.svg\" style=\"width: 1000px; max-width: 80%; margin: 0 auto 0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad8ed7-4d36-4a76-b280-1652090741ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ead7bb-131d-4161-af4c-45776a12faf9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "In the above example (Mont\\'ufar, Pascanu, Cho, & Bengio, 2014), a single curve fits several bends of a horseshoe because the space is folded beneath it.\n",
    "\n",
    "By applying the same adaptive basis function to multiple wiggles, the fitter can find symmetries in the training, producing a fit that generalizes to new data better than fitting each observed wiggle independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d042aea-1cd0-436f-be0d-0b4d0c45ffad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "[Ray Keyes's demo](https://gist.github.com/jpivarski/f99371614ecaa48ace90a6025d430247) of a classifier that twists space in the first layer to make the problem linearly seperable in the next layer.\n",
    "\n",
    "<img src=\"img/network-layer-space-folding.png\" style=\"width: 1200px; max-width: 100%; margin: 0 auto 0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419fb065-75d8-4951-a49b-d0ef1951471d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Deep learning brought neural networks back from the brink of extinction:\n",
    "\n",
    "<img src=\"img/rise-of-deep-learning.svg\" style=\"width: 1200px; max-width: 100%; margin: 0 auto 0 auto\">\n",
    "\n",
    "<br>\n",
    "\n",
    "* 2006‚Äí2007: theoretical improvements\n",
    "* 2012: AlexNet (8 layers) wins ImageNet\n",
    "* 2015: ResNet (152 layers) wins ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e122dde-631b-4841-9117-1ea4cbb15237",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4514b7-a538-4dc6-b38f-d138478e8e1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This talk was intended to show that feed-forward neural networks can be thought of as a generalization of linear fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e67575-8c2a-4bc4-a3e0-adf84d4d7907",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Adaptive basis functions are like Taylor and Fourier series, except that they can be made to fit a function by \"horizontal\" shifting and scaling, not only \"vertical\" scaling with coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c2309-bf76-4fbc-83b3-9f0c0f8c420a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "That horizontal adaptiveness is mathematically equivalent to a hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe811b9-0888-4a1e-8a85-003ce50a6390",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "More layers generalize better because they force the network to search for symmetries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f559c7dc-f504-4ef6-8655-42225bcbffdc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "No dubious analogies to biological brains are required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
